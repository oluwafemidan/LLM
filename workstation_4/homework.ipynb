{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803c5898-de6c-4308-ac48-497764bbd6ad",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "Let's start by getting the dataset. We will use the data we generated in the module.\n",
    "\n",
    "In particular, we'll evaluate the quality of our RAG system\n",
    "with [gpt-4o-mini](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0dacdc3-75cc-45fc-825a-10024a1c9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/04-monitoring/data/results-gpt4o-mini.csv?raw=1'\n",
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cb688ea-6ac1-437b-a979-df608750462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_llm</th>\n",
       "      <th>answer_orig</th>\n",
       "      <th>document</th>\n",
       "      <th>question</th>\n",
       "      <th>course</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>When posting about what you learned from the c...</td>\n",
       "      <td>When you post about what you learned from the ...</td>\n",
       "      <td>f7bc2f65</td>\n",
       "      <td>What tag should I use when posting about my co...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>To read a file with Pandas in Windows, you sho...</td>\n",
       "      <td>How do I read the dataset with Pandas in Windo...</td>\n",
       "      <td>be760b92</td>\n",
       "      <td>Can you show an example of reading a file with...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Yes, you could still receive a certificate eve...</td>\n",
       "      <td>Yes, it's possible. See the previous answer.</td>\n",
       "      <td>1d644223</td>\n",
       "      <td>Will I receive a certificate if I don't comple...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>The mathematical formula for linear regression...</td>\n",
       "      <td>In Question 7 we are asked to calculate\\nThe i...</td>\n",
       "      <td>183a1c90</td>\n",
       "      <td>What is the mathematical formula for linear re...</td>\n",
       "      <td>machine-learning-zoomcamp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            answer_llm  \\\n",
       "105  When posting about what you learned from the c...   \n",
       "154  To read a file with Pandas in Windows, you sho...   \n",
       "45   Yes, you could still receive a certificate eve...   \n",
       "215  The mathematical formula for linear regression...   \n",
       "\n",
       "                                           answer_orig  document  \\\n",
       "105  When you post about what you learned from the ...  f7bc2f65   \n",
       "154  How do I read the dataset with Pandas in Windo...  be760b92   \n",
       "45        Yes, it's possible. See the previous answer.  1d644223   \n",
       "215  In Question 7 we are asked to calculate\\nThe i...  183a1c90   \n",
       "\n",
       "                                              question  \\\n",
       "105  What tag should I use when posting about my co...   \n",
       "154  Can you show an example of reading a file with...   \n",
       "45   Will I receive a certificate if I don't comple...   \n",
       "215  What is the mathematical formula for linear re...   \n",
       "\n",
       "                        course  \n",
       "105  machine-learning-zoomcamp  \n",
       "154  machine-learning-zoomcamp  \n",
       "45   machine-learning-zoomcamp  \n",
       "215  machine-learning-zoomcamp  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[:300]\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac398dc-5d35-4ba5-8858-4e27d5a118f6",
   "metadata": {},
   "source": [
    "## Q1. Getting the embeddings model\n",
    "\n",
    "Now, get the embeddings model `multi-qa-mpnet-base-dot-v1` from\n",
    "[the Sentence Transformer library](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html#model-overview)\n",
    "\n",
    "> Note: this is not the same model as in HW3\n",
    "\n",
    "```bash\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "```\n",
    "\n",
    "Create the embeddings for the first LLM answer:\n",
    "\n",
    "```python\n",
    "answer_llm = df.iloc[0].answer_llm\n",
    "```\n",
    "\n",
    "What's the first value of the resulting vector?\n",
    "\n",
    "* -0.42\n",
    "* -0.22\n",
    "* -0.02\n",
    "* 0.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3031026a-210c-4339-8f83-77de5eb2e77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer_llm     You can sign up for the course by visiting the...\n",
       "answer_orig    Machine Learning Zoomcamp FAQ\\nThe purpose of ...\n",
       "document                                                0227b872\n",
       "question                     Where can I sign up for the course?\n",
       "course                                 machine-learning-zoomcamp\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f78cdb-b2af-4c5a-a12d-54e9f82e4306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can sign up for the course by visiting the course page at [http://mlzoomcamp.com/](http://mlzoomcamp.com/).'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb8dab0-5252-493a-951c-a6ef209319b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_llm = df.iloc[0].answer_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc0061e2-90f9-40bf-873f-1791c66b4502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7101605-89af-4203-9ad2-d0f7e87a296f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.42"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_vector = embedding_model.encode(answer_llm)\n",
    "resulting_vector = round(float(resulting_vector[0]),2)\n",
    "resulting_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c37d1c6-30e5-41d4-b638-e774fc857812",
   "metadata": {},
   "source": [
    "## Q1 Ans: The resulting vector is -0.42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e50b7-3d0b-44a3-9bdf-4add21fa608a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1540cc47-be12-4f35-94a0-d0d9df8c83d1",
   "metadata": {},
   "source": [
    "## Q2. Computing the dot product\n",
    "\n",
    "\n",
    "Now for each answer pair, let's create embeddings and compute dot product between them\n",
    "\n",
    "We will put the results (scores) into the `evaluations` list\n",
    "\n",
    "What's the 75% percentile of the score?\n",
    "\n",
    "* 21.67\n",
    "* 31.67\n",
    "* 41.67\n",
    "* 51.67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae0508ab-6c4e-4cd5-9f2b-cfff9b110ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [02:24<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean      27.495996\n",
      "std        6.384742\n",
      "min        4.547924\n",
      "25%       24.307844\n",
      "50%       28.336870\n",
      "75%       31.674309\n",
      "max       39.476013\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def similarity(rec, model):\n",
    "    orig_ans = rec['answer_orig']\n",
    "    llm_ans = rec['answer_llm']\n",
    "\n",
    "    vec_orig = model.encode(orig_ans)\n",
    "    vec_llm = model.encode(llm_ans)\n",
    "\n",
    "    return vec_orig.dot(vec_llm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for index, rec in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    product = similarity(rec, embedding_model)\n",
    "    evaluations.append(product)\n",
    "\n",
    "\n",
    "\n",
    "evaluations_series = pd.Series(evaluations)\n",
    "summary = evaluations_series.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b4d03-2671-48ba-8e39-b243f4d2fb1d",
   "metadata": {},
   "source": [
    "## Q2 Ans: The 75% percentile of the score is 31.67\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eecb8a-6f24-4ecd-9dd9-faeb22b85c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b18881-5e79-4efc-bc38-a4378128aecb",
   "metadata": {},
   "source": [
    "## Q3. Computing the cosine\n",
    "\n",
    "From Q2, we can see that the results are not within the [0, 1] range. It's because the vectors coming from this model are not normalized.\n",
    "\n",
    "So we need to normalize them.\n",
    "\n",
    "To do it, we \n",
    "\n",
    "* Compute the norm of a vector\n",
    "* Divide each element by this norm\n",
    "\n",
    "So, for vector `v`, it'll be `v / ||v||`\n",
    "\n",
    "In numpy, this is how you do it:\n",
    "\n",
    "```python\n",
    "norm = np.sqrt((v * v).sum())\n",
    "v_norm = v / norm\n",
    "```\n",
    "\n",
    "Let's put it into a function and then compute dot product \n",
    "between normalized vectors. This will give us cosine similarity\n",
    "\n",
    "What's the 75% cosine in the scores?\n",
    "\n",
    "* 0.63\n",
    "* 0.73\n",
    "* 0.83\n",
    "* 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ad7f943-ecd6-4a94-b266-9ed0746c78f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [02:25<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    300.000000\n",
      "mean       0.728393\n",
      "std        0.157755\n",
      "min        0.125357\n",
      "25%        0.651273\n",
      "50%        0.763761\n",
      "75%        0.836235\n",
      "max        0.958796\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Normalize(v):\n",
    "    norm = np.sqrt((v * v).sum())\n",
    "    v_norm = v / norm\n",
    "    return v_norm\n",
    "\n",
    "def Normalize_Similarity(rec, model):\n",
    "    orig_ans = rec['answer_orig']\n",
    "    llm_ans = rec['answer_llm']\n",
    "\n",
    "    vec_orig = model.encode(orig_ans)\n",
    "    vec_llm = model.encode(llm_ans)\n",
    "\n",
    "    vec_orig = Normalize(vec_orig)\n",
    "    vec_llm  = Normalize(vec_llm)\n",
    "\n",
    "    return vec_orig.dot(vec_llm)\n",
    "\n",
    "\n",
    "Normalize_evaluations = []\n",
    "\n",
    "for index, rec in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    product = Normalize_Similarity(rec, embedding_model)\n",
    "    Normalize_evaluations.append(product)\n",
    "\n",
    "\n",
    "Normalize_evaluations_series = pd.Series(Normalize_evaluations)\n",
    "summary = Normalize_evaluations_series.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b484f75-738f-452e-9827-9bcf9b1ff501",
   "metadata": {},
   "source": [
    "## Q3 Ans: The 75% percentile of the score is 0.83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7e24ab-3413-4de2-837d-46d196f4323f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857bc901-0985-4a9f-bdd8-0d8773cd49b2",
   "metadata": {},
   "source": [
    "## Q4. Rouge\n",
    "\n",
    "Now we will explore an alternative metric - the ROUGE score.  \n",
    "\n",
    "This is a set of metrics that compares two answers based on the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "It can give a more nuanced view of text similarity than just cosine similarity alone.\n",
    "\n",
    "We don't need to implement it ourselves, there's a python package for it:\n",
    "\n",
    "```bash\n",
    "pip install rouge\n",
    "```\n",
    "\n",
    "(The latest version at the moment of writing is `1.0.1`)\n",
    "\n",
    "Let's compute the ROUGE score between the answers at the index 10 of our dataframe (`doc_id=5170565b`)\n",
    "\n",
    "```\n",
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "scores = rouge_scorer.get_scores(r['answer_llm'], r['answer_orig'])[0]\n",
    "```\n",
    "\n",
    "There are three scores: `rouge-1`, `rouge-2` and `rouge-l`, and precision, recall and F1 score for each.\n",
    "\n",
    "* `rouge-1` - the overlap of unigrams,\n",
    "* `rouge-2` - bigrams,\n",
    "* `rouge-l` - the longest common subsequence\n",
    "\n",
    "What's the F score for `rouge-1`?\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da612c11-662e-4b00-a98c-7f4383379a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge_scorer = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ac8715a-834e-4bc5-af92-09585e3e17d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.125, 'p': 0.3181818181818182, 'f': 0.17948717543721246},\n",
       " 'rouge-2': {'r': 0.01694915254237288,\n",
       "  'p': 0.038461538461538464,\n",
       "  'f': 0.023529407518339866},\n",
       " 'rouge-l': {'r': 0.10714285714285714,\n",
       "  'p': 0.2727272727272727,\n",
       "  'f': 0.1538461497961868}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_scorer.get_scores(rec['answer_llm'], rec['answer_orig'])[0]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4730aae7-1be8-4a64-bb54-5c0111d5ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45454544954545456"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_scorer.get_scores(df[df['document']=='5170565b']['answer_llm'], df[df['document']=='5170565b']['answer_orig'])[0]\n",
    "scores['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af4379-8d6c-4f90-9ff8-2d2158889906",
   "metadata": {},
   "source": [
    "## Ans 4: The F score for rouge-1 is 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e52e9-8b75-40c9-af77-b4789e0b5757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "744b7c0b-2b74-4dbc-9332-a7987938583b",
   "metadata": {},
   "source": [
    "## Q5. Average rouge score\n",
    "\n",
    "Let's compute the average between `rouge-1`, `rouge-2` and `rouge-l` for the same record from Q4\n",
    "\n",
    "- 0.35\n",
    "- 0.45\n",
    "- 0.55\n",
    "- 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6fdf03cc-6d0d-43a7-afcd-e920814ea73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'r': 0.45454545454545453,\n",
       "  'p': 0.45454545454545453,\n",
       "  'f': 0.45454544954545456},\n",
       " 'rouge-2': {'r': 0.21621621621621623,\n",
       "  'p': 0.21621621621621623,\n",
       "  'f': 0.21621621121621637},\n",
       " 'rouge-l': {'r': 0.3939393939393939,\n",
       "  'p': 0.3939393939393939,\n",
       "  'f': 0.393939388939394}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c92bb46f-2aae-447d-88e5-de0ef28e7c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = rouge_scorer.get_scores(df[df['document']=='5170565b']['answer_llm'], df[df['document']=='5170565b']['answer_orig'])[0]\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_3 = scores['rouge-l']['f']\n",
    "\n",
    "average = np.mean([rouge_1,rouge_2,rouge_3])\n",
    "average = round(float(average),2)\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87236fda-ed29-4b65-af80-4ac7ce86c64d",
   "metadata": {},
   "source": [
    "## Ans 5: The average between rouge-1, rouge-2 and rouge-l is 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffad52-70b2-408d-b1e1-88fb0df90d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9e0fa2d-412d-4154-abea-3e272ba76110",
   "metadata": {},
   "source": [
    "## Q6. Average rouge score for all the data points\n",
    "\n",
    "Now let's compute the score for all the records\n",
    "\n",
    "```python\n",
    "rouge_1 = scores['rouge-1']['f']\n",
    "rouge_2 = scores['rouge-2']['f']\n",
    "rouge_l = scores['rouge-l']['f']\n",
    "rouge_avg = (rouge_1 + rouge_2 + rouge_l) / 3\n",
    "```\n",
    "\n",
    "And create a dataframe from them\n",
    "\n",
    "What's the average `rouge_2` across all the records?\n",
    "\n",
    "- 0.10\n",
    "- 0.20\n",
    "- 0.30\n",
    "- 0.40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "48787d92-4da0-4240-bc86-a919c96eeaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [00:01<00:00, 284.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ROUGE-2 score across all records is: 0.20696501983423318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_rouge_scores(row):\n",
    "    scores = rouge_scorer.get_scores(row['answer_orig'], row['answer_llm'])[0]\n",
    "    return pd.Series({\n",
    "        'rouge-1': scores['rouge-1']['f'],\n",
    "        'rouge-2': scores['rouge-2']['f'],\n",
    "        'rouge-l': scores['rouge-l']['f'],\n",
    "        'rouge-avg': (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) / 3\n",
    "    })\n",
    "\n",
    "tqdm.pandas()\n",
    "scores_df = df.progress_apply(calculate_rouge_scores, axis=1)\n",
    "\n",
    "average_rouge_2 = scores_df['rouge-2'].mean()\n",
    "print(f\"The average ROUGE-2 score across all records is: {average_rouge_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b40052-9c0e-4a19-86e8-d4faf236094a",
   "metadata": {},
   "source": [
    "## Ans 6: The average ROUGE-2 score across all records is: 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ab70c-6c9d-4046-83b3-17d415b21f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
